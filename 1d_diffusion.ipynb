{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion vs Gradient Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from loss_functions import TV_loss, TVV_loss, diffusion_loss, grad_diffusion_loss, robust_diffusion_loss, robust_grad_diffusion_loss\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "cmap = plt.cm.RdYlBu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "momentum=0.99\n",
    "image = torch.ones(1,1,3,100)\n",
    "lr = 0.15\n",
    "rand = torch.randn(1,1,3,100)\n",
    "case1 = torch.sin(torch.linspace(0,6,100)).view(1,1,1,100).expand(1,1,3,100)\n",
    "case2 = -1 + 2*torch.abs(torch.linspace(-1,1,100))\n",
    "iter_per_line = 10\n",
    "lines = 100\n",
    "\n",
    "def optimize(tensor, loss_function, weight, momentum, lr, kappa, loss_name, function_name, ax, lines=lines, iter_per_line=iter_per_line):\n",
    "    pcf = ax.pcolormesh([[0,1],[1,0]],cmap=cmap,vmin=0,vmax=lines*iter_per_line)\n",
    "    ax.clear()\n",
    "    ax.plot(tensor[0,0,0].detach().numpy(),color=cmap(0), label='intial value')\n",
    "    param = torch.nn.Parameter(tensor)\n",
    "    optimizer = torch.optim.SGD([param], lr=lr, momentum=momentum)\n",
    "    for i in tqdm(range(lines)):\n",
    "        for j in range(iter_per_line):\n",
    "            optimizer.zero_grad()\n",
    "            loss = weight*loss_function(param, image, kappa)\n",
    "            loss = loss.mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if loss.item() != loss.item():\n",
    "            break\n",
    "        ax.plot(tensor[0,0,1].detach().numpy(), color=cmap(i/lines))\n",
    "    ax.set_title(\"{} loss on {}\\nlr = {}, momentum = {}\".format(loss_name, function_name, lr, momentum))\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    cbar = fig.colorbar(pcf,cax=cax, ticks = [0,lines*iter_per_line])\n",
    "    cbar.set_ticklabels(['0','1e+{}'.format(int(np.log10(lines*iter_per_line)))])\n",
    "    cbar.ax.tick_params(axis='y', direction='in')\n",
    "    cbar.set_label('# of iterations', rotation=270)\n",
    "\n",
    "\n",
    "fig,axes = plt.subplots(4,2,figsize=(15,20),dpi=200,\n",
    "                        sharex=True,sharey=True)\n",
    "\n",
    "\n",
    "optimize(case1 + 0.2*rand, TV_loss, 1, 0.9, lr, 50, \"TV\", \"a sinusoid\", axes[0,0])\n",
    "axes[0,0].legend()\n",
    "optimize(case1 + 0.2*rand, diffusion_loss, 50, 0.9, lr, 50, \"Diffusion\", \"Sinusoid\", axes[1,0])\n",
    "optimize(case1 + 0.2*rand, TVV_loss, 1, 0.9, lr, 50, \"TVV\", \"a sinusoid\", axes[2,0], iter_per_line = 100)\n",
    "optimize(case1 + 0.2*rand, grad_diffusion_loss, 100, 0.99, lr, 50, \"Gradient diffusion\", \"a sinusoid\", axes[3,0], iter_per_line=100)\n",
    "\n",
    "\n",
    "optimize(case2 + 0.2*rand, TV_loss, 1, 0.9, lr, 50, \"TV\", \"Abs function\", axes[0,1])\n",
    "optimize(case2 + 0.2*rand, diffusion_loss, 50, 0.9, lr, 50, \"Diffusion\", \"Abs function\", axes[1,1])\n",
    "optimize(case2 + 0.2*rand, TVV_loss, 4, 0.9, lr, 50, \"TVV\", \"Abs function\", axes[2,1], iter_per_line=100)\n",
    "optimize(case2 + 0.2*rand, grad_diffusion_loss, 100, 0.99, lr, 50, \"Gradient diffusion\", \"Abs function\", axes[3,1], iter_per_line=100)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum=0.9\n",
    "lr = 0.1\n",
    "rand = torch.randn(100)\n",
    "case1 = 1.5 + (torch.sin(torch.linspace(0,6,100))).view(1,1,1,100).expand(1,1,3,100)\n",
    "case2 = 2.5*(torch.abs(torch.linspace(-1,1,100))).view(1,1,1,100).expand(1,1,3,100)\n",
    "case3 = 2.5*(torch.linspace(1,0,100)).view(1,1,1,100).expand(1,1,3,100)\n",
    "#tensor[:,:,:,20:80] += 0.2\n",
    "image = torch.ones(1,1,3,100)\n",
    "image[:,:,:,:19] = 0\n",
    "image[:,:,:,80:] = 0\n",
    "iter_per_line = 10\n",
    "lines = 100\n",
    "\n",
    "fig,axes = plt.subplots(3,2,figsize=(15,15),dpi=200,\n",
    "                        sharex=True,sharey=True)\n",
    "\n",
    "optimize(case1 + 0*rand, diffusion_loss, 100, 0.9, lr, 0.1, \"Anisotropic diffusion\", \"a sinusoid\", axes[0,0], lines=100)\n",
    "optimize(case2 + 0*rand, diffusion_loss, 100, 0.9, lr, 0.1, \"Anisotropic diffusion\", \"Abs function\", axes[1,0], lines=100)\n",
    "optimize(case3 + 0*rand, diffusion_loss, 100, 0.9, lr, 0.1, \"Anisotropic diffusion\", \"a linear function\", axes[2,0], lines=100)\n",
    "optimize(case1 + 0*rand, grad_diffusion_loss, 100, 0.99, lr, 0.1, \"Anisotropic gradient diffusion\", \"a sinusoid\", axes[0,1], lines=100, iter_per_line=100)\n",
    "optimize(case2 + 0*rand, grad_diffusion_loss, 100, 0.99, lr, 0.1, \"Anisotropic gradient diffusion\", \"Abs function\", axes[1,1], lines=100, iter_per_line=100)\n",
    "optimize(case3 + 0*rand, grad_diffusion_loss, 100, 0.99, lr, 0.1, \"Anisotropic gradient diffusion\", \"a linear function\", axes[2,1], lines=100, iter_per_line=100)\n",
    "for ax in axes:\n",
    "    for a in ax:\n",
    "        a.plot(image[0,0,0].numpy()*0.5, '--', label='image value')\n",
    "axes[0,0].legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diff Loss vs Robust diff loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum=0.99\n",
    "rand = torch.randn(100).clamp(min=-0.3)\n",
    "randomness = 0.1\n",
    "num_lines = 10\n",
    "iter_btw_lines = 1000\n",
    "iter_per_line = 10\n",
    "lines = 100\n",
    "image = torch.ones(1,1,3,100)\n",
    "\n",
    "#optimizer = torch.optim.Adam([param], lr=0.05)\n",
    "\n",
    "def optimize_inverse(tensor, loss_function, weight, momentum, lr, kappa,\n",
    "                     loss_name, function_name, ax1, ax2, lines=lines, iter_per_line=iter_per_line,\n",
    "                     gamma=0, iterations=0, inverse=False):\n",
    "    pcf = ax1.pcolormesh([[0,1],[1,0]],cmap=cmap,vmin=0,vmax=lines*iter_per_line)\n",
    "    ax1.clear()\n",
    "    ax1.plot(tensor[0,0,0].detach().numpy(),color=cmap(0), label='intial value')\n",
    "    ax2.plot(1/tensor[0,0,0].detach().numpy(),color=cmap(0), label='intial value')\n",
    "    param = torch.nn.Parameter(tensor)\n",
    "    optimizer = torch.optim.SGD([param], lr=lr, momentum=momentum)\n",
    "    for i in tqdm(range(lines)):\n",
    "        for j in range(iter_per_line):\n",
    "            optimizer.zero_grad()\n",
    "            to_smooth = 1/param if inverse else param\n",
    "            if gamma==0:\n",
    "                loss = weight*loss_function(to_smooth, image, kappa)\n",
    "            else:\n",
    "                loss = weight*loss_function(to_smooth, image, kappa, gamma, iterations, loss_function='abs')\n",
    "            loss = loss.mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if loss.item() != loss.item():\n",
    "            break\n",
    "        ax1.plot(tensor[0,0,1].detach().numpy(), color=cmap(i/lines))\n",
    "        ax2.plot(1/tensor[0,0,1].detach().numpy(), color=cmap(i/lines))\n",
    "    ax1, ax2 = (ax1, ax2) if inverse else (ax2, ax1)\n",
    "    ax1.set_title(\"{} loss on \\n{}, lr = {}, momentum = {}\".format(loss_name, function_name, lr, momentum))\n",
    "    divider = make_axes_locatable(ax1)\n",
    "    divider2 = make_axes_locatable(ax2)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    _ = divider2.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    _.remove()\n",
    "    cbar = fig.colorbar(pcf,cax=cax, ticks = [0,lines*iter_per_line])\n",
    "    cbar.set_ticklabels(['0','1e+{}'.format(int(np.log10(lines*iter_per_line)))])\n",
    "    cbar.ax.tick_params(axis='y', direction='in')\n",
    "    cbar.set_label('# of iterations', rotation=270, labelpad=-17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.1\n",
    "fig,axes = plt.subplots(2,3,figsize=(20,10),dpi=100,\n",
    "                        sharex=True,sharey=False)\n",
    "\n",
    "case = (2 + torch.sin(torch.linspace(0,6,100))).view(1,1,1,100).expand(1,1,3,100)\n",
    "optimize_inverse(1/(case + randomness*rand), grad_diffusion_loss, 100, 0.99, lr, 500, \"Regular gradient diffusion\", \"an inverse sinusoid\", axes[1,0], axes[0,0], lines=100, iter_per_line=100)\n",
    "optimize_inverse(case + randomness*rand, grad_diffusion_loss, 100, 0.99, lr, 500, \"Regular inverse (unstable) gradient diffusion\", \"a sinusoid\", axes[0,1], axes[1,1], lines=100, iter_per_line=100, inverse=True)\n",
    "optimize_inverse(case + randomness*rand, robust_grad_diffusion_loss, 1, 0.99, lr, 500, \"robust inverse gradient diffusion\", \"a sinusoid\", axes[0,2], axes[1,2], lines=100, iter_per_line=100, inverse=True, gamma=0.3, iterations=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diff vs Robust Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=15\n",
    "rand = torch.randn(100).clamp(min=-10.6)\n",
    "fig,axes = plt.subplots(2,3,figsize=(20,10),dpi=100,\n",
    "                        sharex=True,sharey=False)\n",
    "\n",
    "case = (2 + torch.sin(torch.linspace(0,6,100))).view(1,1,1,100).expand(1,1,3,100)\n",
    "optimize_inverse(1/(case + randomness*rand), diffusion_loss, 1, 0.9, lr, 500, \"Regular gradient diffusion\", \"an inverse sinusoid\", axes[1,0], axes[0,0], lines=100, iter_per_line=10)\n",
    "optimize_inverse(case + randomness*rand, diffusion_loss, 10, 0.9, lr, 500, \"Regular inverse (unstable) diffusion\", \"a sinusoid\", axes[0,1], axes[1,1], lines=100, iter_per_line=10, inverse=True)\n",
    "optimize_inverse(case + randomness*rand, robust_diffusion_loss, 0.01, 0.9, lr, 500, \"robust inverse diffusion\", \"a sinusoid\", axes[0,2], axes[1,2], lines=100, iter_per_line=10, inverse=True, gamma=0.3, iterations=10)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
